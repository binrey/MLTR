{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "from easydict import EasyDict\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from ml import train\n",
    "from dataloading import get_data, build_features\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "cfgs, btests = [], []\n",
    "for p in sorted(Path(\"optimization\").glob(\"*.pickle\")):\n",
    "    cfg, btest = pickle.load(open(p, \"rb\"))\n",
    "    cfgs.append(cfg)\n",
    "    btests.append(btest)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backtest import DataParser, MovingWindow\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "fsize = 64\n",
    "tfdict = {\"M5\":0, \"M15\":1, \"H1\":2}\n",
    "X, y, poslist = [], [], []\n",
    "for btest in btests:\n",
    "    print(btest.cfg.ticker, end=\" \")\n",
    "    hist_pd, hist = DataParser(btest.cfg).load()\n",
    "    mw = MovingWindow(hist, fsize+2)\n",
    "    print(len(btest.positions))\n",
    "    for pos in btest.positions[4:]:\n",
    "        f, _ = mw(pos.open_indx)\n",
    "        x = build_features(f, \n",
    "                           pos.dir, \n",
    "                           btest.cfg.stops_processor.func.cfg.sl, \n",
    "                           btest.cfg.trailing_stop_rate,\n",
    "                           pos.open_date, \n",
    "                           tfdict[btest.cfg.period])\n",
    "        X.append([x])\n",
    "        y.append(pos.profit)\n",
    "        poslist.append(pos)\n",
    "        \n",
    "X, y = np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "print(X.shape, y.shape)\n",
    "print(f\"{X[0, 0, -2, 0]:8.0f} -> {X[-1, 0, -2, 0]:8.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_bin_labels(y, dp=0.3):\n",
    "    y = y.astype(float)\n",
    "    d = np.random.uniform(0, dp, sum(y[:, 0]==0))\n",
    "    y[y[:, 0]==0, 0] = d\n",
    "    y[y[:, 1]==1, 1] = 1-d\n",
    "    return y\n",
    "\n",
    "def smooth_prob_labels(y, dp=0.1):\n",
    "    y = y.astype(float)\n",
    "    d = np.random.uniform(-dp, dp, sum(y[:, 0]==0))\n",
    "    y += dp\n",
    "    return y\n",
    "\n",
    "def stohastic_prediction(input_tensor):\n",
    "    pmeans, pstds, ymeans = np.zeros((3, input_tensor.shape[0]))\n",
    "    for i, x in enumerate(input_tensor):\n",
    "        p = model(torch.stack([x]*100)).detach().cpu().numpy()[:, 0]\n",
    "        pmeans[i] = p.mean()\n",
    "        pstds[i] = p.std()\n",
    "    return pmeans, pstds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "device = \"mps\"\n",
    "\n",
    "nitrers = 100\n",
    "test_split_size = 0.2\n",
    "calc_test = True\n",
    "id2tf = {v:k for k, v in tfdict.items()}\n",
    "\n",
    "pprofits, gprofits, metrics = np.zeros((3, nitrers)), np.zeros((3, nitrers)), np.zeros((2, nitrers))\n",
    "for i in range(nitrers):\n",
    "    if i >= 0:\n",
    "        np.random.seed(i)\n",
    "        X_train, X_test, y_train, y_test, profs_train, profs_test, tf_test = get_data(X, y, test_split_size)\n",
    "    X_train = torch.tensor(X_train).float().to(device)\n",
    "    X_test = torch.tensor(X_test).float().to(device)\n",
    "    model = train(X_train, y_train, X_test, y_test, batch_size=512, device=device, calc_test=calc_test)\n",
    "    model.eval()\n",
    "    p_test = model(X_test).detach().cpu().numpy().squeeze()[:, 0]\n",
    "    p_train = model(X_train[:y_test.shape[0]]).detach().cpu().numpy().squeeze()[:, 0]    \n",
    "    # for m in range(9):\n",
    "    #     np.random.seed(i+10)\n",
    "    #     model = train(X_train, y_train, X_test, smooth_prob_labels(y_test), batch_size=256, device=device, calc_test=False)\n",
    "    #     model.eval()\n",
    "    #     p_test_ = model(X_test).detach().cpu().numpy().squeeze()[:, 0]\n",
    "    #     p_train_ = model(X_train[:y_test.shape[0]]).detach().cpu().numpy().squeeze()[:, 0] \n",
    "    #     p_test = (p_test + p_test_)/2\n",
    "    #     p_train = (p_train + p_train_)/2\n",
    "        \n",
    "    threshold = 0.5\n",
    "    # profsum_best, threshold = -999999, None\n",
    "    # for th in np.arange(0.1, 1, 0.025):\n",
    "    #     # profsum = (profs_train*(p_train>th)).sum()\n",
    "    #     profsum = f1_score(p_train>th, y_train[:, 0])\n",
    "    #     if profsum > profsum_best:\n",
    "    #         profsum_best = profsum\n",
    "    #         threshold = th\n",
    "    metrics[0, i] = f1_score(p_train>threshold, y_train[:y_test.shape[0], 0]>threshold)\n",
    "    print(f\"{i:03} f1_train: {metrics[0, i]:5.3f}\", end=\" \")\n",
    "    \n",
    "    if test_split_size > 0:\n",
    "        metrics[1, i] = f1_score(p_test>threshold, y_test[:, 0]>threshold)\n",
    "        print(f\"f1_test: {metrics[1, i]:5.3f}\")\n",
    "        w_profs_test = np.clip(p_test+0.4, 0, 1)\n",
    "        for j in range(3):\n",
    "            ids = tf_test == j\n",
    "            pprofits[j, i] = (profs_test[ids]*w_profs_test[ids]).sum()\n",
    "            gprofits[j, i] = profs_test[ids].sum()\n",
    "            print(f\"{i:03} profit {id2tf[j]:3}:{gprofits[j, i]:+7.1f} -> {pprofits[j, i]:+7.1f} {'OK' if pprofits[j, i] > gprofits[j, i] else '--'}\")\n",
    "        \n",
    "        pprofs_mean = pprofits.mean(axis=1, where=pprofits!=0)\n",
    "        gprofs_mean = gprofits.mean(axis=1, where=gprofits!=0)\n",
    "        f1_mean = metrics.mean(axis=1, where=metrics!=0)\n",
    "        pprofs_mean_tot = np.nanmean(pprofs_mean)\n",
    "        gprofs_mean_tot = np.nanmean(gprofs_mean)\n",
    "        print(f\"f1_train: {f1_mean[0]:4.2f} f1_test: {f1_mean[1]:4.2f} ratio: {f1_mean[0]/f1_mean[1]:4.2f}\")\n",
    "        print(gprofs_mean, \" -> \", pprofs_mean)\n",
    "        print(\"-------------------------------------------------\")\n",
    "        print(f\"av. profit boost: {(pprofs_mean_tot - gprofs_mean_tot)/abs(gprofs_mean_tot)}\")\n",
    "        metrics_mean, metrics_std = metrics.mean(1, where=metrics!=0), metrics.std(1, where=metrics!=0)\n",
    "        print(f\"metrics train/val means: {metrics_mean}, std: {metrics_std}\")\n",
    "        print(f\"overfitting mean: {metrics_mean[0]/metrics_mean[1]}, std: {metrics_std[0]/metrics_std[1]}\\n\")\n",
    "\n",
    "    else:\n",
    "        print()\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(p_train[:100], \".\")\n",
    "plt.bar(np.arange(100), y_train[:100, 0], width=[1]*100, alpha=0.4)\n",
    "plt.plot([0, 100], [threshold, threshold])\n",
    "if len(p_test):\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(p_test[:100], \".\")\n",
    "    plt.bar(np.arange(100), y_test[:100, 0], width=[1]*100, alpha=0.4)\n",
    "    # plt.bar(np.arange(100), profs_test[:100], width=[1]*100, alpha=0.2)\n",
    "    plt.plot([0, 100], [threshold, threshold])\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(w_profs_test[:100])\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(profs_test[:100]*w_profs_test[:100])\n",
    "    plt.plot(profs_test[:100], linewidth=3, alpha=0.5)  \n",
    "    \n",
    "    \n",
    "model.set_threshold(threshold)\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ml import Net\n",
    "device = \"mps\"\n",
    "model = Net(7, 64)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "# model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stohastic_prediction(input_tensor):\n",
    "    pmeans, pstds, ymeans = np.zeros((3, input_tensor.shape[0]))\n",
    "    for i, x in enumerate(input_tensor):\n",
    "        p = model(torch.stack([x]*100)).detach().cpu().numpy()[:, 0]\n",
    "        pmeans[i] = np.median(p)\n",
    "        pstds[i] = p.std()\n",
    "        ymeans[i] = (pmeans[i] > 0.48) == y_test[i, 0]\n",
    "    return pmeans, pstds, ymeans\n",
    "\n",
    "pmeans, pstds, ymeans = stohastic_prediction(torch.tensor(X_test[:100]).float().to(device))\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(pmeans, \".\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(pstds, \".\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(ymeans, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprofits[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprofits[2].mean(where=pprofits[2]!=0), pprofits[2].std(where=pprofits[2]!=0), (pprofits[2]/gprofits[2]).mean(where=pprofits[2]!=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median((pprofits[2, :-10]/gprofits[2, :-10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pprofits[2]/gprofits[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train.mean(), threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ml import Net\n",
    "device = \"cuda\"\n",
    "model = Net(7, 32)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()\n",
    "# model.set_threshold(-6)\n",
    "model.to(device)\n",
    "X_train, X_test, y_train, y_test, profs_train, profs_test, tf_test = get_data(X, y, test_split=1)\n",
    "p_test = model(torch.tensor(X_test).float().to(device)).squeeze()\n",
    "# profs_test.sum(), (profs_test*p_test).sum()\n",
    "p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.named_parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(model(torch.tensor(X_test).float().to(device)).squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.sum(), (p_test>threshold).sum(), p_test.shape[0], y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mplfinance as mpf\n",
    "\n",
    "ticker = \"BTCUSDT\"\n",
    "tf = \"H1\"\n",
    "hist_pd, hist = DataParser(\n",
    "    EasyDict(\n",
    "        date_start=\"2008-01-01\",\n",
    "        period=tf,\n",
    "        ticker=ticker,\n",
    "        data_type=\"bitfinex\"\n",
    "        )).load()\n",
    "\n",
    "for i in ids_test:\n",
    "    pos = poslist[i]\n",
    "    if pos.ticker == ticker:\n",
    "        prediction = model.predict_proba([X[i, :-1]])[0][1]\n",
    "        if prediction < threshold:\n",
    "            print(pos.ticker, pos.open_date, prediction)\n",
    "            d2 = pd.to_datetime(pos.close_date)\n",
    "            d1 = pd.to_datetime(pos.open_date)\n",
    "            d0 = d1 - pd.DateOffset(days=3)\n",
    "            hist2plot = hist_pd.loc[d0:d2]\n",
    "            fig = mpf.plot(hist2plot, \n",
    "                type='candle', \n",
    "                block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fee188bb3a88b2f91c21b23bf544ec11647f6207f57c38bc925e29c2adb397fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
