{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import PyConfig\n",
    "from tqdm import tqdm\n",
    "from ml.models import autoregress_sequense, batch_sequense\n",
    "from e2e_train import E2ETrain\n",
    "from torchinfo import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from backtest import backtest\n",
    "from loguru import logger\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, format=\"{level} {message}\", colorize=True, level=\"DEBUG\")\n",
    "\n",
    "def save_model(model):\n",
    "    model.eval()\n",
    "    example = torch.rand((1,) + model.inp_shape)\n",
    "    model_jit = torch.jit.trace(model, example)\n",
    "    model_jit.save(\"model.pt\")\n",
    "\n",
    "    # torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = PyConfig(\"zz.py\").test()\n",
    "# cfg.date_start = \"2020.09.01\"\n",
    "# cfg.date_end = \"2018.01.01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_train = E2ETrain(cfg)\n",
    "e2e_train.load_data(dataset_root=\"/Users/andrybin/Yandex.Disk.localized/fin_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "model = e2e_train.train(num_epochs=3000, \n",
    "                        resume=False,\n",
    "                        device=DEVICE,\n",
    "                        val_size=val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output_seq, result_seq, fee_seq = batch_sequense(model, e2e_train.p, e2e_train.features, output_sequense=True, device=DEVICE)\n",
    "hold = e2e_train.compute_hold()\n",
    "\n",
    "fig, ax1 = plt.subplots(2, 1, height_ratios = [3, 1], figsize=(15, 10))\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "ax1.plot(result_seq.cumsum(0))\n",
    "ax1.plot(fee_seq.cumsum(0))\n",
    "ax1.plot(hold, linewidth=3)\n",
    "# ax1.plot(e2e_train.p - e2e_train.p[0], linewidth=3)\n",
    "\n",
    "ax1.plot([e2e_train.p.shape[0]*(1-val_size), e2e_train.p.shape[0]*(1-val_size)], \n",
    "         [0, max(result_seq.cumsum(0).max(), hold.max())],\n",
    "         \"--\",\n",
    "         linewidth=3)\n",
    "ax1.grid(\"on\")\n",
    "plt.tight_layout()\n",
    "plt.grid(\"on\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(list(range(output_seq.shape[0])), height=output_seq, width=1, alpha=0.4)\n",
    "\n",
    "ax1 = plt.subplot(2, 1, 2)\n",
    "ax1.fill(np.hstack([result_seq.cumsum() - hold, np.zeros(1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_train.model(torch.from_numpy(e2e_train.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():   \n",
    "    model = torch.load(\"model.pt\")\n",
    "    output = model(torch.from_numpy(e2e_train.features[300:301]))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "cfg = PyConfig(\"zz.py\").test()\n",
    "date_start = datetime.strptime(cfg.date_start, '%Y-%m-%dT%H:%M:%S').strftime('%m/%d/%Y')\n",
    "date_end = datetime.strptime(cfg.date_end, '%Y-%m-%dT%H:%M:%S').strftime('%m/%d/%Y')\n",
    "\n",
    "years = [pd.to_datetime(d).date().strftime('%Y-%m-%dT%H:%M:%S') for d in \n",
    "         pd.date_range(start=date_start, end=date_end, freq=\"Y\")]\n",
    "\n",
    "date_start = cfg.date_start\n",
    "last_prof = 0\n",
    "for year in years[19:-1]:\n",
    "    cfg = PyConfig(\"zz.py\").test()\n",
    "    cfg.date_end = year\n",
    "    e2e_train = E2ETrain(cfg)\n",
    "    e2e_train.load_data()\n",
    "    model = e2e_train.train(num_epochs=200, resume=False, device=DEVICE)\n",
    "    save_model(model)\n",
    "    \n",
    "    cfg = PyConfig(\"zz.py\").test()\n",
    "    cfg.date_start = year\n",
    "    brok_results = backtest(cfg)\n",
    "\n",
    "    plt.plot(brok_results.daily_hist.days, brok_results.daily_hist.profit + last_prof, linewidth=1, color=\"b\", alpha=0.6)\n",
    "    last_prof += brok_results.final_profit\n",
    "    plt.grid(\"on\")\n",
    "    plt.tight_layout()\n",
    "    # legend.append(f\"{cfg.date_start}-{cfg.date_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brok_results = backtest(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmyolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
